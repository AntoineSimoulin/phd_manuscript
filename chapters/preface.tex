\chapter*{}
% \addcontentsline{toc}{chapter}{Preface} % Add the preface to the table of contents as a chapter

\paragraph{Titre :} Plongements de phrases et leurs relations avec les structures de phrases

\paragraph{Résumé (court) :} Historiquement, la modélisation du langage humain suppose que les phrases ont une structure symbolique et que cette structure permet d’en calculer le sens par composition. Ces dernières années, les modèles d’apprentissage profond sont parvenus à traiter automatiquement des tâches sans s’appuyer sur une structure explicite du langage, remettant ainsi en question cette hypothèse fondamentale. Cette thèse cherche ainsi à mieux identifier le rôle de la structure lors de la modélisation du langage par des modèles d’apprentissage profonds. Elle se place dans le cadre spécifique de la construction de plongements de phrases—des représentations sémantiques basées sur des vecteurs—par des réseaux de neurones profonds. Dans un premier temps, on étudie l’intégration de biais linguistiques dans les architectures de réseaux neuronaux, pour contraindre leur séquence de composition selon une structure traditionnelle, en arbres. Dans un second temps, on relâche ces contraintes pour analyser les structures latentes induites par ces réseaux neuronaux. Dans les deux cas, on analyse les propriétés de composition des modèles ainsi que les propriétés sémantiques des plongements. La thèse s’ouvre sur un état de l’art présentant les principales méthodes de représentation du sens des phrases, qu’elles soient symboliques, ou basées sur des méthodes d’apprentissage profond. La deuxième partie propose plusieurs expériences introduisant des biais linguistiques dans les architectures des réseaux de neurones pour construire des plongements de phrases. Le premier chapitre combine explicitement plusieurs structures de phrases pour construire des représentations sémantiques. Le deuxième chapitre apprend conjointement des structures symboliques et des représentations vectorielles. Le troisième chapitre introduit un cadre formel pour les transformers selon une structure de graphes. Finalement, le quatrième chapitre étudie l’impact de la structure vis-à-vis de la capacité de généralisation et de compositions des modèles. La thèse se termine par une mise en concurrence de ces approches avec des méthodes de passage à l’échelle. On cherche à y discuter les tendances actuelles qui privilégient des modèles plus gros, plus facilement parallélisables et entraînés sur plus de données, aux dépens de modélisations plus fines. Les deux chapitres de cette partie relatent l'entraînement de larges modèles de traitement automatique du langage et comparent ces approches avec celles développées dans la deuxième partie d’un point de vue qualitatif et quantitatif.

\paragraph{Résumé (long) :} Historiquement, la modélisation du langage humain suppose que les phrases ont une structure symbolique et que cette structure permet d’en calculer le sens par composition. Ces dernières années, les modèles d’apprentissage profond sont parvenus à traiter automatiquement des tâches sans s’appuyer sur une structure explicite du langage, remettant ainsi en question cette hypothèse fondamentale. Cette thèse cherche ainsi à mieux identifier le rôle de la structure lors de la modélisation du langage par des modèles d’apprentissage profonds. Elle se place dans le cadre spécifique de la construction de plongements de phrases—des représentations sémantiques basées sur des vecteurs—par des réseaux de neurones profonds. Dans un premier temps, nous étudions l’intégration de biais linguistiques dans les architectures de réseaux neuronaux, pour contraindre leur séquence de composition selon une structure traditionnelle, en arbres. Dans un second temps, nous relâchons ces contraintes pour analyser les structures latentes induites par ces réseaux neuronaux. Dans les deux cas, nous analysons les propriétés de composition des modèles ainsi que les propriétés sémantiques des plongements. La thèse est financée par Quantmetry—un cabinet de conseil français pionnier dans le domaine de l'intelligence artificielle—qui travaille sur des projets d'IA de bout en bout, de la stratégie à l'industrialisation. Ainsi, les travaux sont motivés par un désir de proposer des contributions actionnables dans le milieu industrielles et des outils efficaces pour des applications concrètes. Une grande partie de ce travail est donc publiée sous forme d’outils et de contributions en accès libre.

La thèse s’ouvre sur un état de l’art présentant les principales méthodes de représentation du sens des phrases, qu’elles soient symboliques, ou basées sur des méthodes d’apprentissage profond. 

La deuxième partie propose plusieurs expériences introduisant des biais linguistiques dans les architectures des réseaux de neurones pour construire des plongements de phrases. Cette partie comporte quatre chapitres.

Le premier chapitre combine explicitement plusieurs structures de phrases pour construire des représentations sémantiques. Nous supposons que la signification d'une phrase est une fonction des aspects syntaxiques et sémantiques. À cet égard, nous proposons une méthode auto-supervisée qui construit des plongements de phrases à partir de la combinaison de diverses structures syntaxiques. La nouveauté consiste à proposer une approche multi-vue qui apprend conjointement des modèles structurés en induisant une interaction explicite entre eux pendant la phase d'apprentissage. Nous pré-entraînons plusieurs modèles en utilisant un objectif contrastif avec un corpus de 40 millions de phrases. Nous évaluons ensuite nos modèles sur des ressources d’évaluation des plongements de phrases et obtenons des résultats à l’état de l’art. En particulier sur des tâches qui devraient, par hypothèse, être plus sensibles à la structure des phrases. 

Le deuxième chapitre apprend conjointement des structures symboliques et des représentations vectorielles. Nous utilisons des réseaux neuronaux structurés en arbre, qui encodent naturellement la structure du langage. Pour chaque phrase, le réseau encode les unités de texte en suivant un arbre syntaxique, en partant des feuilles jusqu'à la racine. Cependant, ces modèles souffrent de contraintes pratiques qui limitent leur application. En particulier, les modèles structurés nécessitent non seulement du texte brut en entrée mais aussi la structure de la phrase sous la forme d'un arbre. Une telle structure nécessite des annotations dans le cadre supervisé. Nous formulons un nouveau modèle structuré en arbre qui apprend sa fonction de composition en même temps que sa structure. Le modèle comprend deux modules, un analyseur de graphe biaffine et un Tree-LSTM. Les fonctions d'analyse syntaxique et de composition sont explicitement connectées et, par conséquent, apprises conjointement. La méthode diffère d’autres approches car la représentation n'est pas calculée à partir d’une forêt entière d'arbres potentiels. De plus, l'apprentissage du modèle ne nécessite pas de supervision directe pour la structure. Le modèle est plus performant que les modèles à base d'arbres reposant sur des structures extrinsèques. Dans certaines configurations, il est même compétitif avec Bert.

Le troisième chapitre introduit un cadre formel pour les transformers selon une structure de graphes. Les architectures de transformers ont gagné en popularité au sein de la communauté. Contrairement aux modèles basés sur des arbres, ils n'ont pas besoin de données annotées pour être entraînés. D'autre part, comme le suggèrent de nombreux résultats, ces nouveaux modèles acquièrent une sorte de structure hiérarchique. Les transformers transforment simultanément l’ensemble des représentations des \textit{tokens}—les unités lexicales d’une phrase—selon un nombre fixe de couches. Néanmoins, le rôle de ces couches et la façon dont elles traitent l'information ne sont pas entièrement compris. Nous formulons l'hypothèse que les couches ne codent pas spécifiquement des fonctions surfaciques, syntaxiques ou sémantiques mais plutôt que de telles informations émergent par l'application itérative des couches. Pour mieux étudier la transformation des représentations des \textit{tokens} à travers les couches, nous proposons une variante du modèle ALBERT. A l’instar d’ ALBERT, notre modèle partage ses poids entre l’ensemble des couches mais adapte aussi dynamiquement le nombre de couches appliquées à chaque \textit{token}. Nous analysons le processus de transformation des \textit{tokens} selon la profondeur du réseau. En particulier, nous étudions comment les itérations sont distribuées en fonction des types de dépendance des \textit{tokens}. Nous montrons que les \textit{tokens} ne nécessitent pas le même nombre d'itérations et que les \textit{tokens} difficiles ou cruciaux pour la tâche nécessitent plus d'itérations.

Finalement, le quatrième chapitre étudie l’impact de la structure vis-à-vis de la capacité de généralisation et de compositions des modèles. Bien que les transformers améliorent les performances sur de nombreux \textit{benchmarks}, ils présentent également certaines limites. En particulier en ce qui concerne leur capacité à généraliser en dehors de leur domaine d'entraînement et à apprendre des règles de composition élémentaires. Le \textit{benchmark} COGS par exemple, met en évidence que les modèles d'apprentissage profond ont du mal à généraliser à des séquences plus longues ou à des phrases présentant des niveaux de récusions plus profonds que ceux vus pendant l'entraînement. Pour faire suite à nos travaux sur l'intégration de la structure dans les architectures neuronales, nous cherchons à mieux caractériser le rôle de la structure dans les propriétés compositionnelles des modèles. Ce travail est actuellement en phase d'expérimentation. Nous construisons une méthode d'évaluation avec des expressions arithmétiques contenant des propriétés spécifiques. Nous entraînons différents modèles sur des sous-ensembles du jeu de données et observons comment les modèles généralisent en dehors de leur domaine. Nous comparons des modèles intégrants divers niveaux de contraintes structurelles : des modèles séquentiels, récursifs ou non structurés.

La thèse se termine par une mise en concurrence de ces approches avec des méthodes de passage à l’échelle. Dans cette seconde partie, on cherche à discuter les tendances actuelles qui privilégient des modèles plus gros, plus facilement parallélisables et entraînés sur plus de données, aux dépens de modélisations plus fines. Les deux chapitres de cette partie relatent l'entraînement de larges modèles de traitement automatique du langage et comparent ces approches avec celles développées dans la deuxième partie d’un point de vue qualitatif et quantitatif. 

Le premier chapitre s’interroge sur la taille des modèles. à première vue, il semble que le traitement actuel du langage naturel évolue vers des modèles de plus en plus gros aux dépends des subtilités de leur architecture. Cette tendance n’a pas directement profité aux modèles de plongements de phrases, car de nombreux encodeurs à base de transformers affichent des performances inférieures à l'état de l'art sur les \textit{benchmarks} d’évaluation. Dans cette section, nous explorons comment nous pouvons faire évoluer les performances des encodeurs de phrases en adaptant leur pré-entraînement et en augmentant leur taille. Nous détaillons le développement, l'entraînement et le partage de modèles de plongements de phrases à l’état de l’art. Nous utilisons un objectif contrastif et entraînons les modèles sur un corpus d'un milliard de phrases. 

Le second chapitre de cette partie s’intéresse à l’entrainement d’un modèle de langue incrémental en français. Ce type de modèle peut acquérir des compétences grammaticales très impressionnantes. Par exemple, GPT-2 génère un texte correct avec un accord au pluriel et à distance, et ce, en dépit toute connaissance linguistique préalable. Ces accords sont pourtant déterminés par des structures abstraites et pas seulement par l'ordre linéaire des mots. Plus largement, les modèles peuvent apprendre de nombreux motifs linguistiques (sujet-verbe, nom-adverbe, verbe-verbe) sans aucune information préalable sur la théorie linguistique. Au sein de notre laboratoire, nous avons dirigé le projet d'entraînement du premier grand modèle de langage incrémental en français. Nous avons obtenu une subvention de calcul pour le calculateur public français Jean Zay. Le modèle, équivalent à GPT-2 en anglais, contient plus d'un milliard de paramètres. Nous construisons un corpus d'entraînement dédié et parallélisons l'entraînement entre plusieurs nœuds et unités de calcul. Nous avons publié le modèle en Open-Source pour la recherche et les applications commerciales.

En conclusion, nous avons étudié le rôle de la structure des modèles neuronaux pour composer les unités lexicales lors de la construction de plongements de phrases. Nos travaux ont abordé plusieurs problèmes critiques des plongements de phrases, notamment le manque de robustesse vis-à-vis de la généralisation hors du domaine, le manque de propriétés de compositions, la nécessité de vastes corpus d'entraînement ou la sur paramétrisation. Nos travaux apportent plusieurs contributions. 

Tout d'abord, nous avons donné des éléments empiriques montrant que certaines structures de réseaux neuronaux sont plus appropriées pour capturer des types d'informations spécifiques. Nous avons ainsi observé empiriquement que la structure des modèles impacte la nature des informations accessibles dans les plongements de phrases. Nous avons ainsi confirmé notre hypothèse selon laquelle la combinaison de diverses structures devrait être plus robuste pour les tâches nécessitant des opérations de compositions subtiles.

Deuxièmement, nous avons proposé des architectures originales pour apprendre conjointement la structure de la phrase et la fonction de composition sémantique. Par conséquent, l'apprentissage du modèle ne nécessite pas la supervision d'un objectif d'analyse syntaxique. Nous avons également étudié structures latentes apprises par les modèles transformers et proposé un cadre d’interprétation sous forme de réseaux de graphes. Nos expériences fournissent une nouvelle voie d'interprétation pour le rôle des couches dans les modèles de transformers profonds. Plutôt que d'extraire des caractéristiques spécifiques à chaque étape, les couches pourraient agir comme un processus itératif et convergent.
Troisièmement, nous avons adapté la méthode standard de pré-entraînement contrastive pour entraîner des modèles de transformers de grande taille sur un grand ensemble de données. Nous avons ainsi réussi à étendre leur pré-entraînement pour surpasser les approches précédentes. Toujours dans le cadre du passage à l’échelle, nous avons pré-entraîné une version française du modèle GPT.

Enfin, nous avons développé des ressources d'évaluation. Nous avons développé un jeu de données pour évaluer les propriétés de composition des modèles. En s’appuyant sur des expressions arithmétiques dont nous contrôlons les propriétés, nous évaluons des propriétés qui s'appliquent également à l'étude du langage humain. Nous avons également introduit un jeu de données d'évaluation pour les modèles de langue française.


\paragraph{Mots clefs :} Traitement automatique des langues naturelles, plongements de phrases, apprentissage profond, réseaux de neurones structurés.


\newpage

\paragraph{Title:} Sentence embeddings and their relation with sentence structures

\paragraph{Abstract:} Historically, models of human language assume that sentences have a symbolic structure and that this structure allows us to compute their meaning by composition. In recent years, deep learning models have successfully processed tasks automatically without relying on an explicit language structure, thus challenging this fundamental assumption. This thesis seeks to clearly identify the role of structure in language modeling by deep learning methods. The dissertation specifically investigates the construction of sentence embeddings—semantic representations based on vectors—by deep neural networks. Firstly, we study the integration of linguistic biases in neural network architectures to constrain their composition sequence based on a traditional tree structure. Secondly, we relax these constraints to analyze the latent structures induced by the neural networks. In both cases, we analyze the compositional properties of the models as well as the semantic properties of the sentence embeddings. This thesis begins with an overview of the main methods used to represent the meaning of sentences, either symbolically or using deep learning. The second part proposes several experiments introducing linguistic biases in neural network architectures to build sentence embeddings. The first chapter explicitly combines numerous sentence structures to build semantic representations. The second chapter jointly learns symbolic structures and vector representations. The third chapter introduces a formal framework for graph transformers. Finally, the fourth chapter studies the impact of the structure on the generalization capacity of the models and compares their compositional capabilities. The last part compares the models to larger-scale approaches. It seeks to discuss current trends favoring larger models, more easily parallelized and trained on more data, at the expense of finer modeling. The two chapters from this part report on the training of large models of automatic language processing and compare these approaches with those developed in the second part from a qualitative and quantitative point of view.

\paragraph{Keywords:} Natural language processing, sentence embeddings, deep learning, structured neural networks.

% I am of the opinion that every \LaTeX\xspace geek, at least once during 
% his life, feels the need to create his or her own class: this is what 
% happened to me and here is the result, which, however, should be seen as 
% a work still in progress. Actually, this class is not completely 
% original, but it is a blend of all the best ideas that I have found in a 
% number of guides, tutorials, blogs and tex.stackexchange.com posts. In 
% particular, the main ideas come from two sources:

% \begin{itemize}
% 	\item \href{https://3d.bk.tudelft.nl/ken/en/}{Ken Arroyo Ohori}'s 
% 	\href{https://3d.bk.tudelft.nl/ken/en/nl/ken/en/2016/04/17/a-1.5-column-layout-in-latex.html}{Doctoral 
% 	Thesis}, which served, with the author's permission, as a backbone 
% 	for the implementation of this class;
% 	\item The 
% 		\href{https://github.com/Tufte-LaTeX/tufte-latex}{Tufte-Latex 
% 			Class}, which was a model for the style.
% \end{itemize}

% The first chapter of this book is introductory and covers the most
% essential features of the class. Next, there is a bunch of chapters 
% devoted to all the commands and environments that you may use in writing 
% a book; in particular, it will be explained how to add notes, figures 
% and tables, and references. The second part deals with the page layout 
% and design, as well as additional features like coloured boxes and 
% theorem environments.

% I started writing this class as an experiment, and as such it should be 
% regarded. Since it has always been intended for my personal use, it may
% not be perfect but I find it quite satisfactory for the use I want to 
% make of it. I share this work in the hope that someone might find here 
% the inspiration for writing his or her own class.

% \begin{flushright}
% 	\textit{Federico Marotta}
% \end{flushright}
