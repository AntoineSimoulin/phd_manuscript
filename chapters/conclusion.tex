\setchapterstyle{kao}
\setchapterpreamble[u]{\margintoc}
\chapter{Conclusion and Perspectives}
\labch{conclusion}

\cleanchapterquote{The heart you speak of,’ he said, ‘It might indeed be the hardest part of Josie to learn. It might be like a house with many rooms. Even so, a devoted AF, given time, could walk through each of those rooms, studying them carefully in turn, until they became like her own home.}{Kazuo Ishiguro}{Klara and the Sun}

This dissertation has addressed the topic of sentence embedding and focused on neural model structure's role in composing words into sentence representations. Our contributions addressed several critical issues in sentence embeddings regarding lack of robustness toward out-of-domain generalization, shallow pattern matching rather than compositional knowledge, the requirement for large training datasets, or over-parametrization. We summarize below our key findings.

First, we highlighted that some neural network structures are more appropriate for capturing specific types of information. In the first part of \refch{arithmetics}, we evaluated the ability of models to perform compositional knowledge in a natural language inference task. By comparing distinct structured models and their robustness patterns toward specific linguistic structures, we show they capture distinct types of information. We observed an overall superiority of \bert and identified some of its weaknesses in replacing words with semantic opposites or scrambling words. We take advantage of this observation and jointly learn structured sentence encoders in a contrastive framework. Our results confirm our hypothesis that combining diverse structures should be more robust for tasks requiring performing complex compositional knowledge.

Secondly, we propose original architectures to jointly learn the sentence structure and the semantic composition function. In \refch{tree}, we propose a model consisting of two components: a parser and a \textsc{TreeLSTM} that uses those parses. The parser and composition function are learned jointly and are specific to a given task or domain. Hence, training the full model does not require supervision from a parsing objective. We show that our setup is competitive with \textsc{Bert}-base on a textual similarity task. However, downstream supervision disrupts the production of stable parses and preserving linguistically relevant structures. In \refch{transformers}, we designed an original transformer model that progressively transforms each token through a dynamic number of iterations. We use our model to analyze the role of the layers in deep transformers. We observe patterns across the distribution of iterations and confirm the specific behavior played by special tokens or key tokens for the prediction. Our experiments provide a new interpretation path for the role of layers in deep transformer models. Rather than extracting specific features at each stage, layers could act as an iterative and convergent process.

Thirdly, we complete our goal to propose state-of-the-art sentence encoders by adapting the standard contrastive pre-training method to train large transformer models on a large dataset. While large transformers did not directly meet competitive results on sentence embedding benchmarks, we successfully extended their pre-training to outperform previous approaches. In the domain of large transformer models, we pre-trained a French version of the GPT model from scratch and evaluated it on corresponding benchmarks. While it does not match the raw performance of \textsc{Bert}, its generative properties allow for surprisingly flexible utilization.

Finally, we developed evaluation resources. We introduce CobA, a dataset designed to evaluate model compositional properties. We evaluate properties (localism, substitutivity, productivity, and systematicity) that also apply to the study of human language. We compare encoders with distinct structures: transformers and recurrent or tree-structured models. In general, models are robust toward introducing paraphrases (substitutivity) and can perform the recursive evaluation of sub-components (localism). However, transformers struggle to generalize to longer sequences (productivity) or to combine known parts to form new sequences (systematicity). We also introduced an evaluation dataset for French language models.

Our research could continue in several ways. 

First, we could further investigate the behavior of our iterative transformer model on other datasets. Such in-depth analysis could help us better understand the convergence process within transformers and provide hints to enhance their architectures. 

Secondly, we could integrate other kind of biases into neural network architectures such as symbolic or logic biases. Symbolic AI typically encodes knowledge using explicit rules. These systems may require extensive feature engineering to describe individual elements, but they are very effective at explaining how to compose them. By hard-integrating composition rules, they are naturally more resilient to out-of-domain generalization. Combining symbolic systems and deep learning representation methods is an active subject of research. For example, to combine object recognition and reasoning abilities using generation of symbolic programs or by integrating logic into neural networks. In our opinion, such approach complements methods for intelligibility in deep neural networks. Indeed, we do not attempt to explain models afterward but rather try to constrain their architectures to provide more explicit or readable transformation sequences. 

Finally, our work reveals distinct behaviors of LSTM and transformer models. We proposed a unified framework using graph neural networks. But other kinds of bridges may be considered. For example, we could extend the memory mechanisms to transformers to facilitate convergence across layers.
