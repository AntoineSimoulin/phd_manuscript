\setchapterpreamble[u]{\margintoc}
\chapter{Introduction}
\labch{intro}

\cleanchapterquote{Language is a process of free creation; its laws and principles are fixed, but the manner in which the principles of generation are used is free and infinitely varied. Even the interpretation and use of words involves a process of free creation.}{Noam Chomsky}{Language and Freedom, 1973}

Language is perhaps the most distinguishing characteristic between humans and other animals. Any individual of the homo sapiens species can speak or write and be understood by other members of the species who know that language. As artificial intelligence rises, it remains to be seen if computer programs can "handle" such unique ability.

% Thoughts meaning and language
Whether it is possible to express thoughts, ideas, or meaning through other means than language remains an open question. Some philosophers argue that  it is possible to disentangle thoughts from language.
Locke considers two main functions of language: register our thoughts and share them with others. From his point of view, thoughts may exist independently from words.
Bergson observes that we sometimes have trouble expressing our thoughts with words. In that regard, meaning pre-exists from words, which are only a medium to translate it. From Bergson's point of view, this translation is necessarily inaccurate as words only successfully translate utilitarian, functional, or pragmatic notions but fail to translate intuition, ethereal or ineffable notions.
Others defend that meaning can only emerge through words. For Platon or Merleau-Ponty, thoughts are only an internal discussion.


% introduction part 3
\bcomment{The previous sections discussed the importance of neural model structure in composing sentence representations. Along with the architecture of the sentence encoder, other parameters may strongly influence the quality of the embeddings. In particular, the last few years have seen a race to increase the number of parameters, hidden size, or size of pre-training data. As already observed in \refch{arithmetics} or \refsec{survey:downstream}, scaling models is an established method for improving downstream results\sidenote{The effect of scaling is considered so obvious that it was used as the primary reason to reject RoBERTa  paper \parencite{liu_2019} from ICLR 2020. As expressed by the Program Chairs: "most of the findings are obvious (careful tuning helps, more data helps)". \url{https://openreview.net/forum?id=SyxS0T4tvS}}.}{manque de problématique; ne pas mettre cette note en évidence}

\bcomment{
The following sections aims to train and evaluate sentence embedding models at scale. \refch{1B} presents an attempt to train state-of-the-art sentence embedding models on a very large corpus. \refch{generative} propose to train the first large generative pre-trained model in French. Finally, \refch{structure-scale} proposes to train structured models at scale.}{Nope, à mettre dans l'intro ou ailleurs; intro de la partie peut etre; mettre un plan du chapitre à la place}
